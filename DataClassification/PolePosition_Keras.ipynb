{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2psgaQpzqsYb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Funcao para gerar dataset a partir do conjunto de dados\n",
        "def dataframe_2_dataset(dataframe, batch_size):\n",
        "  labels = dataframe.pop(0)\n",
        "  data_array = dataframe.to_numpy()\n",
        "  matrices = [data_array[i].reshape((10, 10, 1)) for i in range(len(dataframe))]\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((matrices, labels))\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "oHdhAUmSrDkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def criar_modelo_cnn(hp):\n",
        "  # Camada Entrada\n",
        "  input = tf.keras.Input(shape=(10, 10, 1))\n",
        "\n",
        "  # Camada Convolucional 1\n",
        "  hp_conv1_filters = hp.Int('hp_conv1_filters', min_value=4, max_value=64, step=4)\n",
        "  conv1 = tf.keras.layers.Conv2D(hp_conv1_filters, (3, 3), activation='relu')(input)\n",
        "\n",
        "  # Camada Convolucional 2\n",
        "  hp_conv2_filters = hp.Int('hp_conv2_filters', min_value=4, max_value=128, step=4)\n",
        "  conv2 = tf.keras.layers.Conv2D(hp_conv2_filters, (3, 3), activation='relu')(conv1)\n",
        "\n",
        "  # Camada Convolucional 3\n",
        "  hp_conv3_filters = hp.Int('hp_conv3_filters', min_value=4, max_value=128, step=4)\n",
        "  conv3 = tf.keras.layers.Conv2D(hp_conv3_filters, (3, 3), activation='relu')(conv2)\n",
        "\n",
        "  # Camada MaxPooling\n",
        "  max_pool = tf.keras.layers.MaxPooling2D((2, 2))(conv3)\n",
        "\n",
        "  # Camada Flatten\n",
        "  flatten = tf.keras.layers.Flatten()(max_pool)\n",
        "\n",
        "  # Camada Densa\n",
        "  hp_dense_units = hp.Int('hp_dense_units', min_value=64, max_value=256, step=8)\n",
        "  dense = tf.keras.layers.Dense(hp_dense_units, activation='relu')(flatten)\n",
        "\n",
        "  # Camada Sa√≠da\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
        "\n",
        "  # Combina as camadas em um modelo sequencial\n",
        "  model = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "  # Define a razao de aprendizado\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5])\n",
        "\n",
        "  # Compila o modelo, define o otimizador, a funcao de perda e as metricas\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
        "                 loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                 metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "hR8w3y3WvoyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega e aleatoriza o conjunto de treinamento\n",
        "dataframe_treinamento= pd.read_csv('/pole/datasets/treinamento.csv', header=None).sample(frac=1, random_state=32)\n",
        "print('Quantidade de dados de treinamento: ', len(dataframe_treinamento))\n",
        "\n",
        "# Separa 10% dos dados de treinamento para a busca\n",
        "dataframe_busca_treinamento = dataframe_treinamento.copy().sample(frac=0.10, random_state=32)\n",
        "print('Quantidade de dados de treinamento para busca: ', len(dataframe_busca_treinamento))\n",
        "\n",
        "# Separa 10% dos dados de treinamento da busca (separados acima) para a validacao\n",
        "dataframe_busca_validacao = dataframe_busca_treinamento.copy().sample(frac=0.10, random_state=32)\n",
        "print('Quantidade de dados de validacao para busca: ', len(dataframe_busca_validacao))\n",
        "\n",
        "# Cria o dataset de treinamento e validacao\n",
        "dataset_busca_treinamento = dataframe_2_dataset(dataframe_busca_treinamento, 64)\n",
        "dataset_busca_validacao = dataframe_2_dataset(dataframe_busca_validacao, 64)"
      ],
      "metadata": {
        "id": "FhBgYFAmrHX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4c347b-9f30-4332-bcdf-faa5690994f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantidade de dados de treinamento:  15264\n",
            "Quantidade de dados de treinamento para busca:  1526\n",
            "Quantidade de dados de validacao para busca:  153\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria o KerasTurner com os parametros indicados\n",
        "tuner = kt.Hyperband(hypermodel=criar_modelo_cnn,\n",
        "                     objective='val_binary_accuracy',\n",
        "                     max_epochs=1000,\n",
        "                     directory='keras_test_45',\n",
        "                     project_name='pole_position')\n",
        "\n",
        "# Define a funcao de parada monitorando o valor de perda da validacao\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)"
      ],
      "metadata": {
        "id": "7Hv0mxcBr7vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a busca dos hiper-parametros\n",
        "tuner.search(dataset_busca_treinamento, validation_data=dataset_busca_validacao, epochs=1000, callbacks=[stop_early])"
      ],
      "metadata": {
        "id": "IA9zgyq-9ZEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7039980-cbf9-4c51-a72f-5d9fcd3b1ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2074 Complete [00h 01m 35s]\n",
            "val_binary_accuracy: 0.9673202633857727\n",
            "\n",
            "Best val_binary_accuracy So Far: 1.0\n",
            "Total elapsed time: 03h 52m 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtem os melhores parametros encontrados\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print('hp_conv1_filters: ', best_hps.get('hp_conv1_filters'))\n",
        "print('hp_conv2_filters: ', best_hps.get('hp_conv2_filters'))\n",
        "print('hp_conv3_filters: ', best_hps.get('hp_conv3_filters'))\n",
        "print('hp_dense_units: ', best_hps.get('hp_dense_units'))\n",
        "print('learning_rate: ', best_hps.get('learning_rate'))\n",
        "print('tuner/epochs: ', best_hps.get('tuner/epochs'))"
      ],
      "metadata": {
        "id": "adoaj-dGAlpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69bccebe-e387-40b0-f780-7968c68bc430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hp_conv1_filters:  48\n",
            "hp_conv2_filters:  124\n",
            "hp_conv3_filters:  44\n",
            "hp_dense_units:  216\n",
            "learning_rate:  0.001\n",
            "tuner/epochs:  334\n"
          ]
        }
      ]
    }
  ]
}